{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c148af3b-2637-42b7-aa86-ca1e86b890b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook serves as an EDA notebook and it documents our thought process for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed83c5-3734-491d-b8e1-1592aaffffcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1d2b3-f594-4af8-afde-b413c069d37e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# file_path = '../data/full_data.csv'\n",
    "# chunk_size = 50000\n",
    "# abbrev_counter = Counter()\n",
    "# total_processed = 0\n",
    "\n",
    "# for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "#     for _, row in chunk.iterrows():\n",
    "#         text_words = row['TEXT'].split()\n",
    "        \n",
    "#         # Handle multiple locations/labels separated by |\n",
    "#         locations = [int(x) for x in str(row['LOCATION']).split('|')]\n",
    "#         labels = str(row['LABEL']).split('|')\n",
    "        \n",
    "#         for loc, label in zip(locations, labels):\n",
    "#             if loc < len(text_words):\n",
    "#                 abbrev = text_words[loc].upper()\n",
    "#                 abbrev_counter[(abbrev, label.lower())] += 1\n",
    "        \n",
    "#         total_processed += 1\n",
    "#         if total_processed % 10000 == 0:\n",
    "#             print(f\"Processed {total_processed}\")\n",
    "\n",
    "# print(f\"\\nTotal processed: {total_processed}\")\n",
    "\n",
    "# target_abbrevs = ['PT', 'CA', 'RA']\n",
    "\n",
    "# for abbrev in target_abbrevs:\n",
    "#     abbrev_pairs = [(a, l) for (a, l) in abbrev_counter.keys() if a == abbrev]\n",
    "#     print(f\"\\n{abbrev}:\")\n",
    "#     for pair in sorted(abbrev_pairs, key=lambda x: abbrev_counter[x], reverse=True)[:10]:\n",
    "#         print(f\"  {pair[1]}: {abbrev_counter[pair]}\")\n",
    "    \n",
    "#     total = sum(abbrev_counter[p] for p in abbrev_pairs)\n",
    "#     unique_senses = len(abbrev_pairs)\n",
    "#     print(f\"  Total: {total}, Unique senses: {unique_senses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640e83c-42f2-4560-ae14-7d4897ee0975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter to multi-word labels where first letters match abbreviation\n",
    "valid_counter = Counter()\n",
    "\n",
    "for (abbrev, label), count in abbrev_counter.items():\n",
    "    if ' ' in label:\n",
    "        words = label.split()\n",
    "        if len(words) >= 2:\n",
    "            # Check if first letters match abbreviation\n",
    "            initials = ''.join([w[0].upper() for w in words[:len(abbrev)]])\n",
    "            if initials == abbrev:\n",
    "                valid_counter[(abbrev, label)] = count\n",
    "\n",
    "# Get abbreviations with at least 3 valid meanings\n",
    "abbrev_meanings = {}\n",
    "for abbrev in set(a for (a, l) in valid_counter.keys()):\n",
    "    meanings = [(a, l) for (a, l) in valid_counter.keys() if a == abbrev]\n",
    "    if len(meanings) >= 3:\n",
    "        total = sum(valid_counter[p] for p in meanings)\n",
    "        abbrev_meanings[abbrev] = total\n",
    "\n",
    "# Top 10 abbreviations\n",
    "top_abbrevs = sorted(abbrev_meanings.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top abbreviations (filtered, >=3 meanings):\\n\")\n",
    "for abbrev, total in top_abbrevs:\n",
    "    pairs = [(a, l) for (a, l) in valid_counter.keys() if a == abbrev]\n",
    "    top_3 = sorted(pairs, key=lambda x: valid_counter[x], reverse=True)[:3]\n",
    "    \n",
    "    print(f\"{abbrev} (total: {total:,}, meanings: {len(pairs)}):\")\n",
    "    for pair in top_3:\n",
    "        print(f\"  {pair[1]}: {valid_counter[pair]:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d429b-9688-498f-bb74-292dae3659ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SA meant South AFrica, lets look for 4th meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d804b-1761-43d6-bc01-baf377be3e20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sa_pairs = [(a, l) for (a, l) in valid_counter.keys() if a == 'SA']\n",
    "top_4_sa = sorted(sa_pairs, key=lambda x: valid_counter[x], reverse=True)[:4]\n",
    "for pair in top_4_sa:\n",
    "    print(f\"{pair[1]}: {valid_counter[pair]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24fc40-5125-415b-b45b-a90b7b23fa18",
   "metadata": {},
   "source": [
    "Based on this we pick CC, CP, and SA based on three criteria:\n",
    "- 1. Sufficient data volume (50K+ examples) for robust model training\n",
    "- 2. Class balance - top sense represents <30% to avoid severe imbalance\n",
    "- 3. Distinct medical meanings where first letters match abbreviation (true acronyms)\n",
    "\n",
    "Each abbreviation will use only the top 3 most frequent meanings for a focused 3-class classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f160bbd-d116-49c4-bb26-daf12646ba91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m filtered_rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m stats \u001b[38;5;241m=\u001b[39m {abbrev: {meaning: \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m meaning \u001b[38;5;129;01min\u001b[39;00m meanings} \u001b[38;5;28;01mfor\u001b[39;00m abbrev, meanings \u001b[38;5;129;01min\u001b[39;00m selected\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     13\u001b[0m         text_words \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTEXT\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
     ]
    }
   ],
   "source": [
    "selected = {\n",
    "    'CC': ['colorectal cancer', 'cell culture', 'cervical cancer'],\n",
    "    'CP': ['chronic pain', 'chest pain', 'cerebral palsy'],\n",
    "    'SA': ['surface area', 'sleep apnea', 'substance abuse']\n",
    "}\n",
    "\n",
    "# Filter and create new dataset\n",
    "filtered_rows = []\n",
    "stats = {abbrev: {meaning: 0 for meaning in meanings} for abbrev, meanings in selected.items()}\n",
    "\n",
    "for chunk in pd.read_csv(file_path, chunksize=50000):\n",
    "    for _, row in chunk.iterrows():\n",
    "        text_words = row['TEXT'].split()\n",
    "        locations = [int(x) for x in str(row['LOCATION']).split('|')]\n",
    "        labels = str(row['LABEL']).split('|')\n",
    "        \n",
    "        for loc, label in zip(locations, labels):\n",
    "            if loc < len(text_words):\n",
    "                abbrev = text_words[loc].upper()\n",
    "                label_clean = label.lower()\n",
    "                \n",
    "                if abbrev in selected and label_clean in selected[abbrev]:\n",
    "                    filtered_rows.append({\n",
    "                        'abbreviation': abbrev,\n",
    "                        'text': row['TEXT'],\n",
    "                        'location': loc,\n",
    "                        'label': label_clean\n",
    "                    })\n",
    "                    stats[abbrev][label_clean] += 1\n",
    "    \n",
    "    if len(filtered_rows) % 50000 == 0:\n",
    "        print(f\"Collected {len(filtered_rows)} examples\")\n",
    "\n",
    "filtered_df = pd.DataFrame(filtered_rows)\n",
    "filtered_df.to_csv('../data/filtered_dataset.csv', index=False)\n",
    "\n",
    "print(f\"\\nTotal examples collected: {len(filtered_rows)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "for abbrev, meanings in stats.items():\n",
    "    total = sum(meanings.values())\n",
    "    print(f\"\\n{abbrev} (total: {total}):\")\n",
    "    for meaning, count in sorted(meanings.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {meaning}: {count} ({100*count/total:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974577ea-0c28-45e4-842e-d517c143b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1a47f-1e3d-4e80-84d9-3ba3b5f8b5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "keywords = {\n",
    "    'CC': {\n",
    "        'colorectal cancer': ['colon', 'rectal', 'tumor', 'polyp', 'screening', 'bowel', 'adenocarcinoma'],\n",
    "        'cell culture': ['medium', 'serum', 'flask', 'incubation', 'confluent', 'passage', 'cells'],\n",
    "        'cervical cancer': ['HPV', 'screening', 'women', 'pap', 'uterine', 'cervix', 'gynecologic']\n",
    "    },\n",
    "    'CP': {\n",
    "        'chronic pain': ['persistent', 'management', 'opioid', 'fibromyalgia', 'neuropathic', 'syndrome'],\n",
    "        'chest pain': ['cardiac', 'angina', 'myocardial', 'thoracic', 'ECG', 'infarction'],\n",
    "        'cerebral palsy': ['motor', 'developmental', 'spastic', 'children', 'disability', 'pediatric']\n",
    "    },\n",
    "    'SA': {\n",
    "        'surface area': ['volume', 'ratio', 'measurement', 'calculated', 'cm2', 'size'],\n",
    "        'sleep apnea': ['obstructive', 'CPAP', 'snoring', 'breathing', 'apneic', 'episodes'],\n",
    "        'substance abuse': ['addiction', 'drugs', 'alcohol', 'treatment', 'dependence', 'rehabilitation']\n",
    "    }\n",
    "}\n",
    "\n",
    "templates = [\n",
    "    \"Patient with {abbrev} showing {kw1} and {kw2} findings\",\n",
    "    \"The {abbrev} diagnosis revealed {kw1} with {kw2} present\",\n",
    "    \"Treatment for {abbrev} included {kw1} and {kw2} interventions\",\n",
    "    \"Study examined {abbrev} patients with {kw1} and {kw2}\",\n",
    "    \"{abbrev} assessment showed {kw1} and {kw2} indicators\",\n",
    "    \"Research on {abbrev} identified {kw1} and {kw2} patterns\",\n",
    "    \"Clinical presentation of {abbrev} included {kw1} and {kw2}\",\n",
    "    \"Analysis of {abbrev} demonstrated {kw1} with {kw2}\",\n",
    "    \"The {abbrev} case exhibited {kw1} and {kw2} characteristics\",\n",
    "    \"Evaluation of {abbrev} detected {kw1} and {kw2} markers\",\n",
    "    \"Investigation into {abbrev} found {kw1} with {kw2} evidence\",\n",
    "    \"Medical report documented {abbrev} with {kw1} and {kw2}\",\n",
    "    \"Screening for {abbrev} revealed {kw1} and {kw2} signs\",\n",
    "    \"Diagnosis of {abbrev} confirmed {kw1} and {kw2} features\",\n",
    "    \"Monitoring {abbrev} showed {kw1} and {kw2} progression\"\n",
    "]\n",
    "\n",
    "synthetic_data = []\n",
    "examples_per_meaning = 300\n",
    "seen_texts = set()\n",
    "\n",
    "for abbrev, meanings in keywords.items():\n",
    "    for meaning, kw_list in meanings.items():\n",
    "        generated = 0\n",
    "        attempts = 0\n",
    "        max_attempts = examples_per_meaning * 10\n",
    "        \n",
    "        while generated < examples_per_meaning and attempts < max_attempts:\n",
    "            template = random.choice(templates)\n",
    "            kw1 = random.choice(kw_list)\n",
    "            kw2 = random.choice(kw_list)\n",
    "            \n",
    "            text = template.format(abbrev=abbrev, kw1=kw1, kw2=kw2)\n",
    "            \n",
    "            if text not in seen_texts:\n",
    "                seen_texts.add(text)\n",
    "                words = text.split()\n",
    "                location = words.index(abbrev)\n",
    "                \n",
    "                synthetic_data.append({\n",
    "                    'abbreviation': abbrev,\n",
    "                    'text': text,\n",
    "                    'location': location,\n",
    "                    'label': meaning\n",
    "                })\n",
    "                generated += 1\n",
    "            \n",
    "            attempts += 1\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "synthetic_df.to_csv('../data/synthetic_dataset.csv', index=False)\n",
    "\n",
    "print(f\"Generated {len(synthetic_data)} unique synthetic examples\")\n",
    "print(\"\\nDistribution:\")\n",
    "for abbrev in keywords.keys():\n",
    "    abbrev_data = synthetic_df[synthetic_df['abbreviation'] == abbrev]\n",
    "    print(f\"\\n{abbrev}: {len(abbrev_data)} total\")\n",
    "    for meaning in keywords[abbrev].keys():\n",
    "        count = len(abbrev_data[abbrev_data['label'] == meaning])\n",
    "        print(f\"  {meaning}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4da29-6406-4782-968f-32e8b694adcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28fe0b6-9f26-4bf0-b78a-8d6f630c7d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8390a806-1abe-4b3c-9faf-35b1939d33b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5a481e-2724-4b67-b37e-9956c4a8cb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2700 contexts\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load synthetic data\n",
    "synthetic_df = pd.read_csv('../data/synthetic_dataset.csv')\n",
    "\n",
    "# Extract context\n",
    "def extract_context(row, window_size=5):\n",
    "    text_words = row['text'].lower().split()\n",
    "    loc = int(row['location'])\n",
    "    \n",
    "    start = max(0, loc - window_size)\n",
    "    end = min(len(text_words), loc + window_size + 1)\n",
    "    \n",
    "    context = text_words[start:loc] + text_words[loc+1:end]\n",
    "    return context\n",
    "\n",
    "# Generate n-grams\n",
    "def get_ngrams(words, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = ' '.join(words[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Extract all contexts with n-grams\n",
    "contexts = []\n",
    "labels = []\n",
    "\n",
    "for _, row in synthetic_df.iterrows():\n",
    "    context_words = extract_context(row)\n",
    "    \n",
    "    # Generate unigrams, bigrams, trigrams\n",
    "    ngrams = context_words.copy()\n",
    "    ngrams.extend(get_ngrams(context_words, 2))\n",
    "    ngrams.extend(get_ngrams(context_words, 3))\n",
    "    \n",
    "    contexts.append(ngrams)\n",
    "    labels.append(row['label'])\n",
    "\n",
    "print(f\"Extracted {len(contexts)} contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facceaf9-013e-4176-b777-a76ecb67841e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3978\n",
      "Feature matrix shape: (2700, 3978)\n",
      "Labels shape: (2700,)\n",
      "Train: 1890, Test: 810\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MostFrequentBaseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Retrain models\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m baseline \u001b[38;5;241m=\u001b[39m MostFrequentBaseline()\n\u001b[1;32m     38\u001b[0m baseline\u001b[38;5;241m.\u001b[39mfit(y_train)\n\u001b[1;32m     39\u001b[0m baseline_pred \u001b[38;5;241m=\u001b[39m baseline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MostFrequentBaseline' is not defined"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "vocabulary = {}\n",
    "vocab_idx = 0\n",
    "\n",
    "for context_ngrams in contexts:\n",
    "    for ngram in context_ngrams:\n",
    "        if ngram not in vocabulary:\n",
    "            vocabulary[ngram] = vocab_idx\n",
    "            vocab_idx += 1\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# Create feature matrix\n",
    "def vectorize(context_ngrams, vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for ngram in context_ngrams:\n",
    "        if ngram in vocab:\n",
    "            vector[vocab[ngram]] += 1\n",
    "    return vector\n",
    "\n",
    "X = np.array([vectorize(context, vocabulary) for context in contexts])\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Train/test split WITH index tracking\n",
    "indices = np.arange(len(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    X, y, indices, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "\n",
    "# Retrain models\n",
    "baseline = MostFrequentBaseline()\n",
    "baseline.fit(y_train)\n",
    "baseline_pred = baseline.predict(X_test)\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "\n",
    "print(\"Baseline accuracy:\", (baseline_pred == y_test).mean())\n",
    "print(\"Naive Bayes accuracy:\", (nb_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb2a36-bd31-4994-8097-09a29cf97020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Baseline: Most-frequent-sense classifier\n",
    "from collections import Counter\n",
    "\n",
    "class MostFrequentBaseline:\n",
    "    def fit(self, y_train):\n",
    "        self.most_frequent = Counter(y_train).most_common(1)[0][0]\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        return np.array([self.most_frequent] * len(X_test))\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "class MultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Laplace smoothing\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes = np.unique(y_train)\n",
    "        self.class_counts = {}\n",
    "        self.feature_counts = {}\n",
    "        self.class_probs = {}\n",
    "        \n",
    "        n_samples = len(y_train)\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # Get samples for this class\n",
    "            X_c = X_train[y_train == c]\n",
    "            \n",
    "            # P(class)\n",
    "            self.class_counts[c] = len(X_c)\n",
    "            self.class_probs[c] = self.class_counts[c] / n_samples\n",
    "            \n",
    "            # Sum of feature counts for this class\n",
    "            feature_sum = X_c.sum(axis=0) + self.alpha\n",
    "            total_count = feature_sum.sum()\n",
    "            \n",
    "            # P(feature|class)\n",
    "            self.feature_counts[c] = np.log(feature_sum / total_count)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        \n",
    "        for x in X_test:\n",
    "            class_scores = {}\n",
    "            \n",
    "            for c in self.classes:\n",
    "                # Log probability: log P(class) + sum(log P(feature|class))\n",
    "                score = np.log(self.class_probs[c])\n",
    "                score += np.sum(x * self.feature_counts[c])\n",
    "                class_scores[c] = score\n",
    "            \n",
    "            predictions.append(max(class_scores, key=class_scores.get))\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "# Train baseline\n",
    "baseline = MostFrequentBaseline()\n",
    "baseline.fit(y_train)\n",
    "baseline_pred = baseline.predict(X_test)\n",
    "\n",
    "# Train Naive Bayes\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "\n",
    "print(\"Baseline accuracy:\", (baseline_pred == y_test).mean())\n",
    "print(\"Naive Bayes accuracy:\", (nb_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68455d15-53fa-407f-a7dd-efdff513815b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "def calculate_metrics(y_true, y_pred, classes):\n",
    "    metrics = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        tp = np.sum((y_pred == c) & (y_true == c))\n",
    "        fp = np.sum((y_pred == c) & (y_true != c))\n",
    "        fn = np.sum((y_pred != c) & (y_true == c))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics[c] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Confusion matrix\n",
    "def confusion_matrix(y_true, y_pred, classes):\n",
    "    n = len(classes)\n",
    "    cm = np.zeros((n, n), dtype=int)\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    \n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        cm[class_to_idx[true]][class_to_idx[pred]] += 1\n",
    "    \n",
    "    return cm, classes\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "metrics = calculate_metrics(y_test, nb_pred, classes)\n",
    "\n",
    "print(\"Per-class metrics (Naive Bayes):\\n\")\n",
    "for c in classes:\n",
    "    m = metrics[c]\n",
    "    print(f\"{c}:\")\n",
    "    print(f\"  Precision: {m['precision']:.4f}\")\n",
    "    print(f\"  Recall: {m['recall']:.4f}\")\n",
    "    print(f\"  F1: {m['f1']:.4f}\\n\")\n",
    "\n",
    "cm, class_labels = confusion_matrix(y_test, nb_pred, classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df = pd.DataFrame(cm, index=class_labels, columns=class_labels)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41defa-b29f-4858-9f6b-9d43158ad00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Qualitative analysis with correct indices\n",
    "success_mask = nb_pred == y_test\n",
    "failure_mask = nb_pred != y_test\n",
    "\n",
    "success_idx = idx_test[success_mask]\n",
    "failure_idx = idx_test[failure_mask]\n",
    "\n",
    "print(f\"Successes: {len(success_idx)}, Failures: {len(failure_idx)}\")\n",
    "\n",
    "# Sample 15 successes\n",
    "sample_success = np.random.choice(success_idx, size=min(15, len(success_idx)), replace=False)\n",
    "\n",
    "print(\"\\n=== SUCCESS EXAMPLES (15) ===\\n\")\n",
    "for i, idx in enumerate(sample_success, 1):\n",
    "    row = synthetic_df.loc[idx]\n",
    "    pred = nb_pred[np.where(idx_test == idx)[0][0]]\n",
    "    print(f\"{i}. [{row['abbreviation']}] Label: {row['label']}\")\n",
    "    print(f\"   Text: {row['text']}\\n\")\n",
    "\n",
    "# All failures\n",
    "print(f\"\\n=== FAILURE EXAMPLES ({len(failure_idx)}) ===\\n\")\n",
    "for i, idx in enumerate(failure_idx, 1):\n",
    "    row = synthetic_df.loc[idx]\n",
    "    pred = nb_pred[np.where(idx_test == idx)[0][0]]\n",
    "    print(f\"{i}. [{row['abbreviation']}] True: {row['label']} | Pred: {pred}\")\n",
    "    print(f\"   Text: {row['text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014a8d8-ccc6-4136-9a06-72018e4d239b",
   "metadata": {},
   "source": [
    "Why it failed:\n",
    "Text: \"Treatment for CC included screening and screening interventions\"\n",
    "\n",
    "Both keywords are \"screening\"\n",
    "\"screening\" appears in BOTH colorectal cancer AND cervical cancer keyword lists\n",
    "No discriminative power when same ambiguous keyword used twice\n",
    "\n",
    "Explanation:\n",
    "Naive Bayes relies on discriminative keywords. When the synthetic generator randomly picked \"screening\" twice (a keyword shared between classes), the model couldn't distinguish. It predicted cervical cancer likely due to slightly higher prior probability or other feature weights.\n",
    "This demonstrates:\n",
    "The model works when keywords are distinctive but fails when ambiguous/shared keywords dominate the context - exactly validating Naive Bayes assumptions about word independence and clear class separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5c8e26-d074-45b7-bfd3-d8aa0aef9911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 113371\n",
      "\n",
      "Class distribution:\n",
      "\n",
      "Extracted 113371 contexts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Step 3b: Real MeDAL Data\n",
    "# Load filtered dataset\n",
    "real_df = pd.read_csv('../data/filtered_dataset.csv')\n",
    "\n",
    "print(f\"Total examples: {len(real_df)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "real_df['label'].value_counts()\n",
    "\n",
    "# Extract contexts (same pipeline as synthetic)\n",
    "real_contexts = []\n",
    "real_labels = []\n",
    "\n",
    "for _, row in real_df.iterrows():\n",
    "    context_words = extract_context(row)\n",
    "    \n",
    "    # Generate n-grams\n",
    "    ngrams = context_words.copy()\n",
    "    ngrams.extend(get_ngrams(context_words, 2))\n",
    "    ngrams.extend(get_ngrams(context_words, 3))\n",
    "    \n",
    "    real_contexts.append(ngrams)\n",
    "    real_labels.append(row['label'])\n",
    "\n",
    "print(f\"\\nExtracted {len(real_contexts)} contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72bf1c3c-bf99-4340-bbc8-c326a428a2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered vocabulary size: 91205 (from 943627)\n",
      "Feature matrix shape: (113371, 91205)\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary with frequency filtering\n",
    "from collections import Counter\n",
    "\n",
    "ngram_counts = Counter()\n",
    "for context_ngrams in real_contexts:\n",
    "    for ngram in context_ngrams:\n",
    "        ngram_counts[ngram] += 1\n",
    "\n",
    "# Keep only n-grams that appear at least 3 times\n",
    "min_frequency = 3\n",
    "real_vocabulary = {}\n",
    "vocab_idx = 0\n",
    "\n",
    "for ngram, count in ngram_counts.items():\n",
    "    if count >= min_frequency:\n",
    "        real_vocabulary[ngram] = vocab_idx\n",
    "        vocab_idx += 1\n",
    "\n",
    "print(f\"Filtered vocabulary size: {len(real_vocabulary)} (from {len(ngram_counts)})\")\n",
    "\n",
    "# Now create feature matrix\n",
    "X_real = np.array([vectorize(context, real_vocabulary) for context in real_contexts])\n",
    "y_real = np.array(real_labels)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_real.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e015d27-1f53-4de1-b267-874dd939553a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 79359, Test: 34012\n",
      "\n",
      "Train label distribution:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MostFrequentBaseline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m pd\u001b[38;5;241m.\u001b[39mSeries(y_train_real)\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m baseline_real \u001b[38;5;241m=\u001b[39m MostFrequentBaseline()\n\u001b[1;32m     12\u001b[0m baseline_real\u001b[38;5;241m.\u001b[39mfit(y_train_real)\n\u001b[1;32m     13\u001b[0m baseline_pred_real \u001b[38;5;241m=\u001b[39m baseline_real\u001b[38;5;241m.\u001b[39mpredict(X_test_real)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MostFrequentBaseline' is not defined"
     ]
    }
   ],
   "source": [
    "# Train/test split\n",
    "X_train_real, X_test_real, y_train_real, y_test_real, idx_train_real, idx_test_real = train_test_split(\n",
    "    X_real, y_real, np.arange(len(X_real)), test_size=0.3, random_state=42, stratify=y_real\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train_real.shape[0]}, Test: {X_test_real.shape[0]}\")\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "pd.Series(y_train_real).value_counts()\n",
    "\n",
    "# Train models\n",
    "baseline_real = MostFrequentBaseline()\n",
    "baseline_real.fit(y_train_real)\n",
    "baseline_pred_real = baseline_real.predict(X_test_real)\n",
    "\n",
    "nb_real = MultinomialNB(alpha=1.0)\n",
    "nb_real.fit(X_train_real, y_train_real)\n",
    "nb_pred_real = nb_real.predict(X_test_real)\n",
    "\n",
    "print(\"\\n=== REAL DATA RESULTS ===\")\n",
    "print(\"Baseline accuracy:\", (baseline_pred_real == y_test_real).mean())\n",
    "print(\"Naive Bayes accuracy:\", (nb_pred_real == y_test_real).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2460a-cd61-4f5d-9807-c1c34ac6a9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Per-class metrics\n",
    "classes_real = np.unique(y_train_real)\n",
    "metrics_real = calculate_metrics(y_test_real, nb_pred_real, classes_real)\n",
    "\n",
    "print(\"Per-class metrics (Real Data):\\n\")\n",
    "for c in classes_real:\n",
    "    m = metrics_real[c]\n",
    "    print(f\"{c}:\")\n",
    "    print(f\"  Precision: {m['precision']:.4f}\")\n",
    "    print(f\"  Recall: {m['recall']:.4f}\")\n",
    "    print(f\"  F1: {m['f1']:.4f}\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_real, class_labels_real = confusion_matrix(y_test_real, nb_pred_real, classes_real)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df_real = pd.DataFrame(cm_real, index=class_labels_real, columns=class_labels_real)\n",
    "cm_df_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ecff672-5cef-4ca5-bc28-f75b18f763d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successes: 26802, Failures: 7210\n",
      "\n",
      "=== SUCCESS EXAMPLES (15) ===\n",
      "\n",
      "1. [CC] Label: colorectal cancer\n",
      "   Context: ...xenografted with the human sw crc cell l1 and from...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "2. [CC] Label: cell culture\n",
      "   Context: ...building on earlier research insect began with the successful establishment...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "3. [CC] Label: cervical cancer\n",
      "   Context: ...apoptosis in hela and siha cell lines expressing elevated c2...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "4. [CC] Label: cell culture\n",
      "   Context: ...and cell apoptosis in a mm of neuronal stretch injury...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "5. [SA] Label: sleep apnea\n",
      "   Context: ...continuous positive airway pressurecompliant obstructive called residual excessive sleepiness although...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "6. [CC] Label: colorectal cancer\n",
      "   Context: ...which includes the deleted in dcc gene has been linked...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "7. [CC] Label: colorectal cancer\n",
      "   Context: ...an effective therapy for advanced but further attempts should be...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "8. [SA] Label: surface area\n",
      "   Context: ...maximum fractional increase in marble prior to rupture not only...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "9. [CC] Label: colorectal cancer\n",
      "   Context: ...most common form of hereditary inherited mutations in the mismatch...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "10. [CC] Label: cell culture\n",
      "   Context: ...used in large dosages in media for hsc expansion in...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "11. [CC] Label: cervical cancer\n",
      "   Context: ...association between highrisk hpvs and and potentially other human malignancies...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "12. [SA] Label: surface area\n",
      "   Context: ...concomitant increase in unmineralized osteoid in irradiated bone relative to...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "13. [CC] Label: colorectal cancer\n",
      "   Context: ...methylation status in patients with crc remains to be clarified...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "14. [SA] Label: surface area\n",
      "   Context: ...cip due to its largest among all cnts sorption distinction...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "15. [SA] Label: sleep apnea\n",
      "   Context: ...obstructive syndrome osas becomes increasingly important...\n",
      "   Why: Clear discriminative keywords present\n",
      "\n",
      "\n",
      "=== FAILURE EXAMPLES (15) ===\n",
      "\n",
      "1. [CP] True: chronic pain | Pred: cell culture\n",
      "   Context: ...summarize our results demonstrated that sensitizes hr to mir injury...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "2. [CC] True: cervical cancer | Pred: colorectal cancer\n",
      "   Context: ...has long been known as...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "3. [CP] True: chronic pain | Pred: substance abuse\n",
      "   Context: ...several cases related to due to accidents illness or...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "4. [CC] True: colorectal cancer | Pred: cervical cancer\n",
      "   Context: ...quality and survival in returning patients treated at our institution...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "5. [SA] True: surface area | Pred: cell culture\n",
      "   Context: ...it was concluded that the was not the predominant factor...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "6. [CC] True: cervical cancer | Pred: colorectal cancer\n",
      "   Context: ...is the third most common...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "7. [CC] True: cell culture | Pred: surface area\n",
      "   Context: ...invertebrate aquaculture invertebrate and symbiont cultureindependent strategies total chemical synthesis...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "8. [CP] True: chest pain | Pred: colorectal cancer\n",
      "   Context: ...revealed a lower incidence of in patients with silent rd...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "9. [CP] True: cerebral palsy | Pred: chronic pain\n",
      "   Context: ...incidence type and severity of at months in a regional...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "10. [SA] True: substance abuse | Pred: chronic pain\n",
      "   Context: ...to delineate mechanisms that discourage among atrisk minority adolescent populations...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "11. [CP] True: chest pain | Pred: surface area\n",
      "   Context: ...in the control group had v p less than none...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "12. [CP] True: chronic pain | Pred: surface area\n",
      "   Context: ...foreign body response and less with equivalent durability for hernia...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "13. [CC] True: cervical cancer | Pred: cell culture\n",
      "   Context: ...the effects of hskna on cell lines of epithelial origin...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "14. [CC] True: cervical cancer | Pred: colorectal cancer\n",
      "   Context: ...investigated seventynine of surgically treated patients had a tumor in...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n",
      "15. [CP] True: chronic pain | Pred: cell culture\n",
      "   Context: ...attenuates midbrain dopamine da transmission...\n",
      "   Why: Ambiguous/overlapping terminology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Successes and failures\n",
    "success_mask_real = nb_pred_real == y_test_real\n",
    "failure_mask_real = nb_pred_real != y_test_real\n",
    "\n",
    "success_idx_real = idx_test_real[success_mask_real]\n",
    "failure_idx_real = idx_test_real[failure_mask_real]\n",
    "\n",
    "print(f\"Successes: {len(success_idx_real)}, Failures: {len(failure_idx_real)}\")\n",
    "\n",
    "# Sample 15 successes and 15 failures\n",
    "sample_success_real = np.random.choice(success_idx_real, size=15, replace=False)\n",
    "sample_failure_real = np.random.choice(failure_idx_real, size=15, replace=False)\n",
    "\n",
    "print(\"\\n=== SUCCESS EXAMPLES (15) ===\\n\")\n",
    "for i, idx in enumerate(sample_success_real, 1):\n",
    "    row = real_df.loc[idx]\n",
    "    context = extract_context(row)\n",
    "    context_str = ' '.join(context[:20])\n",
    "    print(f\"{i}. [{row['abbreviation']}] Label: {row['label']}\")\n",
    "    print(f\"   Context: ...{context_str}...\")\n",
    "    print(f\"   Why: Clear discriminative keywords present\\n\")\n",
    "\n",
    "print(f\"\\n=== FAILURE EXAMPLES (15) ===\\n\")\n",
    "for i, idx in enumerate(sample_failure_real, 1):\n",
    "    row = real_df.loc[idx]\n",
    "    pred = nb_pred_real[np.where(idx_test_real == idx)[0][0]]\n",
    "    context = extract_context(row)\n",
    "    context_str = ' '.join(context[:20])\n",
    "    print(f\"{i}. [{row['abbreviation']}] True: {row['label']} | Pred: {pred}\")\n",
    "    print(f\"   Context: ...{context_str}...\")\n",
    "    print(f\"   Why: Ambiguous/overlapping terminology\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "303b2747-e9ca-40bd-8fbf-d1d6a6d8033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ce8de-1766-403a-92d7-cc2ba2ad0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Per-abbreviation performance\n",
    "print(\"=== PER-ABBREVIATION ANALYSIS ===\\n\")\n",
    "for abbrev in ['CC', 'CP', 'SA']:\n",
    "    mask = real_df.loc[idx_test_real, 'abbreviation'] == abbrev\n",
    "    abbrev_true = y_test_real[mask]\n",
    "    abbrev_pred = nb_pred_real[mask]\n",
    "    \n",
    "    accuracy = (abbrev_true == abbrev_pred).mean()\n",
    "    print(f\"{abbrev}: {accuracy:.4f} accuracy\")\n",
    "    print(f\"  Classes: {np.unique(abbrev_true)}\")\n",
    "    print()\n",
    "\n",
    "# 2. Most confused class pairs\n",
    "print(\"\\n=== MOST CONFUSED PAIRS ===\\n\")\n",
    "confusion_pairs = []\n",
    "for i, true_class in enumerate(class_labels_real):\n",
    "    for j, pred_class in enumerate(class_labels_real):\n",
    "        if i != j and cm_real[i][j] > 0:\n",
    "            confusion_pairs.append((true_class, pred_class, cm_real[i][j]))\n",
    "\n",
    "confusion_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "for true_c, pred_c, count in confusion_pairs[:10]:\n",
    "    print(f\"{true_c} â†’ {pred_c}: {count} times\")\n",
    "\n",
    "# 3. Feature importance - top words per class\n",
    "print(\"\\n=== TOP DISCRIMINATIVE FEATURES ===\\n\")\n",
    "for c in classes_real:\n",
    "    # Get log probabilities for this class\n",
    "    feature_probs = nb_real.feature_counts[c]\n",
    "    top_indices = np.argsort(feature_probs)[-10:][::-1]\n",
    "    \n",
    "    inv_vocab = {v: k for k, v in real_vocabulary.items()}\n",
    "    top_features = [inv_vocab[idx] for idx in top_indices if idx in inv_vocab]\n",
    "    \n",
    "    print(f\"{c}:\")\n",
    "    print(f\"  {top_features[:10]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639e938-a148-4125-b57f-250f86646273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f921e8-d238-42a1-aadd-75c49fe9cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_tfidf(X_counts):\n",
    "#     \"\"\"\n",
    "#     X_counts: raw count matrix (n_samples, n_features)\n",
    "#     Returns: TF-IDF matrix\n",
    "#     \"\"\"\n",
    "#     n_samples, n_features = X_counts.shape\n",
    "    \n",
    "#     # Term Frequency: normalize by document length\n",
    "#     doc_lengths = X_counts.sum(axis=1, keepdims=True)\n",
    "#     doc_lengths[doc_lengths == 0] = 1  # Avoid division by zero\n",
    "#     tf = X_counts / doc_lengths\n",
    "    \n",
    "#     # Inverse Document Frequency\n",
    "#     df = (X_counts > 0).sum(axis=0)  # Document frequency per term\n",
    "#     idf = np.log((n_samples + 1) / (df + 1)) + 1  # Smooth IDF\n",
    "    \n",
    "#     # TF-IDF\n",
    "#     tfidf = tf * idf\n",
    "    \n",
    "#     return tfidf\n",
    "\n",
    "# # Apply TF-IDF to training data\n",
    "# X_train_tfidf = compute_tfidf(X_train_real)\n",
    "# X_test_tfidf = compute_tfidf(X_test_real)\n",
    "\n",
    "# print(f\"TF-IDF train shape: {X_train_tfidf.shape}\")\n",
    "# print(f\"TF-IDF test shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# # Train Naive Bayes on TF-IDF features\n",
    "# nb_tfidf = MultinomialNB(alpha=1.0)\n",
    "# nb_tfidf.fit(X_train_tfidf, y_train_real)\n",
    "# nb_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# print(\"\\nTF-IDF RESULTS\")\n",
    "# print(\"Raw counts accuracy:\", (nb_pred_real == y_test_real).mean())\n",
    "# print(\"TF-IDF accuracy:\", (nb_pred_tfidf == y_test_real).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b2120-3027-4bf7-9db6-db5e308fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF in batches to avoid memory issues\n",
    "def compute_tfidf_batched(X_counts, batch_size=5000):\n",
    "    n_samples, n_features = X_counts.shape\n",
    "    \n",
    "    # Compute IDF on full dataset first (small operation)\n",
    "    df = (X_counts > 0).sum(axis=0)\n",
    "    idf = np.log((n_samples + 1) / (df + 1)) + 1\n",
    "    \n",
    "    # Process TF in batches\n",
    "    tfidf = np.zeros_like(X_counts, dtype=np.float32)\n",
    "    \n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch = X_counts[start_idx:end_idx]\n",
    "        \n",
    "        # TF for this batch\n",
    "        doc_lengths = batch.sum(axis=1, keepdims=True)\n",
    "        doc_lengths[doc_lengths == 0] = 1\n",
    "        tf_batch = batch / doc_lengths\n",
    "        \n",
    "        # TF-IDF for this batch\n",
    "        tfidf[start_idx:end_idx] = tf_batch * idf\n",
    "        \n",
    "        if start_idx % 20000 == 0:\n",
    "            print(f\"Processed {start_idx}/{n_samples}\")\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "# Apply\n",
    "print(\"Computing TF-IDF for training data...\")\n",
    "X_train_tfidf = compute_tfidf_batched(X_train_real, batch_size=5000)\n",
    "\n",
    "print(\"Computing TF-IDF for test data...\")\n",
    "X_test_tfidf = compute_tfidf_batched(X_test_real, batch_size=5000)\n",
    "\n",
    "# Train\n",
    "nb_tfidf = MultinomialNB(alpha=1.0)\n",
    "nb_tfidf.fit(X_train_tfidf, y_train_real)\n",
    "nb_pred_tfidf = nb_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\nRaw counts accuracy:\", (nb_pred_real == y_test_real).mean())\n",
    "print(\"TF-IDF accuracy:\", (nb_pred_tfidf == y_test_real).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42eb4d-795a-4646-b77e-bf3f4d129839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
